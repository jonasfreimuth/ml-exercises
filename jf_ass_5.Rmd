---
title: "R Notebook"
output: html_notebook
---

```{r Setup, include=FALSE}

library("keras")

## function definitions

# function for splitting a data frame into a set of test and training data
#   assumes that the prediction variable is categorical and all predictor
#   variables are numeric
# 
#   the categorical prediction variable is encoded in the one-hot scheme
# 
#   training predictor variables are normalized and scaled to their standard deviation
#   and the test data is normalized and scaled the same way (to training data)
generateTestTraining <- function(df, dep_col, pred_col) {
  
  # extract dependent variable and convert to numeric encoding
  y <- df[[dep_col]] %>% 
    as.numeric()
  
  # extract predictor variables and save in a matrix
  x <- subset(df, select = pred_col) %>% 
    as.matrix()
  
  n     <- nrow(df)
  n_var <- ncol(x)
  n_lvl <- length(unique(y))
  
  
  ## split into training and test data
  
  # generate membership vector for test or training data
  mem_vec <- sample(1:2, n, replace = TRUE, prob = c(0.7, 0.3))
  
  # split data according to membership vector
  x_train <- x[mem_vec == 1, ]
  x_test  <- x[mem_vec == 2, ]
  
  y_train <- y[mem_vec == 1] %>% 
    # we need to subtract one in order for the factor levels to start at 0
    `-`(1) %>% 
    # use one hot encoding, as is provided by keras::to_categorical
    to_categorical(num_classes = n_lvl)
  
  y_test <- y[mem_vec == 2] %>% 
    # we need to subtract one in order for the factor levels to start at 0
    `-`(1) %>% 
    # use one hot encoding, as is provided by keras::to_categorical
    to_categorical(num_classes = n_lvl)
  
  # save original test values for y
  y_test_vec <- y[mem_vec == 2]
  
  n_train <- length(y_train)
  n_test  <- length(y_test )
  
  
  ## normalization of test and training data
  
  # get means and sds of all pred vars for training data
  train_mean <- apply(x_train, 2, mean)
  train_sd   <- apply(x_train, 2, sd)
  
  # center and scale test and training data to mean and sd of training data
  for (i in 1:n_var) {
    
    x_train[ , i] <- scale(x_train[ , i],
                           center = train_mean[i],
                           scale  = train_sd  [i])
    
    x_test [ , i] <- scale(x_test [ , i],
                           center = train_mean[i],
                           scale  = train_sd  [i])
    
  }
  
  out <- list(train = list(x = x_train, y = y_train),
              test  = list(x = x_test , y = y_test , y_orig = y_test_vec))
  
  return(out)
}


train_model <- function(data,
                        optimizer = "adam",
                        input_act = "sigmoid",
                        intermediate_act = "sigmoid",
                        last_act = "softmax",
                        epochs = 500,
                        batch_size = 32,
                        validation_split,
                        verbose = 0) {
  
  n_lvl <- ncol(data$train$y)
  n_var <- ncol(data$train$x)
  
  # set up model
  model <- keras_model_sequential() 
  
  model %>% 
    
    # define input layer
    layer_dense(units       = n_var,
                activation  = input_act,
                input_shape = n_var) %>% 
    
    # define intermediate layers
    layer_dense(units = 10,
                activation = intermediate_act) %>%
    
    
    # define output layer
    # number of nodes/units corresponds to number of levels of dependent variable
    # uses softmax activation for multiclass prediction
    layer_dense(units = ncol(data$train$y),
                activation = last_act)
  
  # specify how model will be run
  model %>% 
    compile(loss      = "binary_crossentropy",
            optimizer = optimizer,
            metrics   = c("accuracy"))
  
  browser()
  
  # train the model
  history <- model %>% 
  fit(x = data$train$x,
      y = data$train$y,
      
      epochs = epochs,
      
      batch_size = batch_size,
      
      validation_split = 0.2,
      verbose = verbose)
  
  # give the final model performance on the testing data
  print(model %>% evaluate(data$test$x, data$test$y, verbose = verbose))

  # extract class probabilities for test data
  prob <- model %>% 
    predict(data$test$x)
  
  # extract predicted classes from probabilities
  predClass <- apply(prob, 1, which.max)
  
  # confusion table
  conf_table <- table(predicted = predClass, actual = data$test$y_orig)
  
  return(list(model = model, history = history, confusionTable = conf_table))
  
}


```

During setup I noticed that training looks quite different when the data is not 
shuffled:

```{r Data preparation, include=FALSE}

# load data
data("iris")

data_orig <- generateTestTraining(iris,
                                  dep_col  = names(iris)[ 5],
                                  pred_col = names(iris)[-5])

data      <- generateTestTraining(iris[sample(1:nrow(iris)), ],
                                  dep_col  = names(iris)[ 5],
                                  pred_col = names(iris)[-5])

```





```{r working model}

train_model(data)

```




