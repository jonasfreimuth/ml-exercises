---
title: "R Notebook"
output: html_notebook
---

```{r Setup, include=FALSE}

library("keras")

## function definitions

generateTestTraining <- function(df, dep_col, pred_col) {
  
  # extract dependent variable and convert to numeric encoding
  y <- df[[dep_col]] %>% 
    as.numeric()
  
  # extract predictor variables and save in a matrix
  x <- subset(df, select = pred_col) %>% 
    as.matrix()
  
  n     <- nrow(df)
  n_var <- ncol(x)
  n_lvl <- length(unique(y))
  
  
  ## split into training and test data
  
  # generate membership vector for test or training data
  mem_vec <- sample(1:2, n, replace = TRUE, prob = c(0.7, 0.3))
  
  # split data according to membership vector
  x_train <- x[mem_vec == 1, ]
  x_test  <- x[mem_vec == 2, ]
  
  y_train <- y[mem_vec == 1] %>% 
    # we need to subtract one in order for the factor levels to start at 0
    `-`(1) %>% 
    # use one hot encoding, as is provided by keras::to_categorical
    to_categorical(num_classes = n_lvl)
  
  y_test <- y[mem_vec == 2] %>% 
    # we need to subtract one in order for the factor levels to start at 0
    `-`(1) %>% 
    # use one hot encoding, as is provided by keras::to_categorical
    to_categorical(num_classes = n_lvl)
  
  # save original test values for y
  y_test_vec <- y[mem_vec == 2]
  
  n_train <- length(y_train)
  n_test  <- length(y_test )
  
  
  ## normalization of test and training data
  
  # get means and sds of all pred vars for training data
  train_mean <- apply(x_train, 2, mean)
  train_sd   <- apply(x_train, 2, sd)
  
  # center and scale test and training data to mean and sd of training data
  for (i in 1:n_var) {
    
    x_train[ , i] <- scale(x_train[ , i],
                           center = train_mean[i],
                           scale  = train_sd  [i])
    
    x_test [ , i] <- scale(x_test [ , i],
                           center = train_mean[i],
                           scale  = train_sd  [i])
    
  }
  
  out <- list(train = list(x = x_train, y = y_train),
              test  = list(x = x_test , y = y_test , y_orig = y_test_vec))
  
  return(out)
}

train_model <- function(data, optimizer = "adam",
                        input_act = "sigmoid",
                        intermediate_act = "sigmoid",
                        last_act = "softmax") {
  
                        }


```

During setup I noticed that training looks quite different when the data is not 
shuffled:

```{r Data preparation, include=FALSE}
# load data
data("iris")

data      <- generateTestTraining(iris,
                                  dep_col  = names(iris)[ 5],
                                  pred_col = names(iris)[-5])

data_orig <- generateTestTraining(iris[sample(1:nrow(iris)), ],
                                  dep_col  = names(iris)[ 5],
                                  pred_col = names(iris)[-5])

```

```{r Model specification, include=FALSE}

# set up model
model <- keras_model_sequential() 

model %>% 
  
  # define input layer
  layer_dense(units       = n_lvl,
              activation  = "sigmoid",
              input_shape = n_var) %>% 
  
  # define intermediate layers
  layer_dense(units = 10,
              activation = 'sigmoid') %>%
  
  
  # define output layer
  # number of nodes/units corresponds to number of levels of dependent variable
  # uses softmax activation for multiclass prediction
  layer_dense(units = ncol(y_train),
              activation = "softmax")

# specify how model will be run
model %>% 
  compile(loss      = "binary_crossentropy",
          optimizer = "adam",
          metrics   = c("accuracy"))

```

```{r Model training, include=FALSE}

history <- model %>% 
  fit(x = x_train,
      y = y_train,
      
      epochs = 500,
      
      batch_size = 32,
      
      validation_split = 0.2,
      verbose = 0)

```

```{r Learning history}

plot(history)

```


```{r Model evaluation}

model %>% evaluate(x_test, y_test, verbose = 0)

# extract class probabilities
prob <- model %>% 
  predict(x_test)

# extract predicted classes
predClass <- apply(prob, 1, which.max)

# confusion table
table(predicted = predClass, actual = y_test_vec)

```



