---
title: "R Notebook"
output: html_notebook
---

```{r Setup, include=FALSE}

library("keras")

# load data
data("iris")

iris <- iris[sample(1:nrow(iris)), ]

```

```{r Data preparation, include=FALSE}
# extract dependent variable and convert to numeric encoding
y <- iris$Species %>% 
  as.numeric()

# extract predictor variables and save in a matrix
x <- subset(iris, select = names(iris)[-5]) %>% 
  as.matrix()

n     <- nrow(iris)
n_var <- ncol(x)
n_lvl <- length(unique(y))


## split into training and test data

# generate membership vector for test or training data
mem_vec <- sample(1:2, n, replace = TRUE, prob = c(0.7, 0.3))

# split data according to membership vector
x_train <- x[mem_vec == 1, ]
x_test  <- x[mem_vec == 2, ]

y_train <- y[mem_vec == 1] %>% 
  # we need to subtract one in order for the factor levels to start at 0
  `-`(1) %>% 
  # use one hot encoding, as is provided by keras::to_categorical
  to_categorical(num_classes = n_lvl)

y_test <- y[mem_vec == 2] %>% 
  # we need to subtract one in order for the factor levels to start at 0
  `-`(1) %>% 
  # use one hot encoding, as is provided by keras::to_categorical
  to_categorical(num_classes = n_lvl)

# save original test values for y
y_test_vec <- y[mem_vec == 2]

n_train <- length(y_train)
n_test  <- length(y_test )


## normalization of test and training data

# get means and sds of all pred vars for training data
train_mean <- apply(x_train, 2, mean)
train_sd   <- apply(x_train, 2, sd)

# center and scale test and training data to mean and sd of training data
for (i in 1:n_var) {
  
  x_train[ , i] <- scale(x_train[ , i],
                         center = train_mean[i],
                         scale  = train_sd  [i])
  
  x_test [ , i] <- scale(x_test [ , i],
                         center = train_mean[i],
                         scale  = train_sd  [i])
  
}

```

```{r Model specification, include=FALSE}

# set up model
model <- keras_model_sequential() 

model %>% 
  
  # define input layer
  layer_dense(units       = n_lvl,
              activation  = "sigmoid",
              input_shape = n_var) %>% 
  
  # define intermediate layers
  layer_dense(units = 10,
              activation = 'sigmoid') %>%
  
  
  # define output layer
  # number of nodes/units corresponds to number of levels of dependent variable
  # uses softmax activation for multiclass prediction
  layer_dense(units = ncol(y_train),
              activation = "softmax")

# specify how model will be run
model %>% 
  compile(loss      = "binary_crossentropy",
          optimizer = "adam",
          metrics   = c("accuracy"))

```

```{r Model training, include=FALSE}

history <- model %>% 
  fit(x = x_train,
      y = y_train,
      
      epochs = 500,
      
      batch_size = 32,
      
      validation_split = 0.2,
      verbose = 0)

```

```{r Learning history}

plot(history)

```


```{r Model evaluation}

model %>% evaluate(x_test, y_test, verbose = 0)

# extract class probabilities
prob <- model %>% 
  predict(x_test)

# extract predicted classes
predClass <- apply(prob, 1, which.max)

# confusion table
table(predicted = predClass, actual = y_test_vec)

```



